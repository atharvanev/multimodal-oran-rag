{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d0f9bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.37.0 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (4.37.0)\n",
      "Requirement already satisfied: filelock in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from transformers==4.37.0) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.0) (1.1.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from requests->transformers==4.37.0) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from requests->transformers==4.37.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from requests->transformers==4.37.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages (from requests->transformers==4.37.0) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers==4.37.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f016ac75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prime/miniconda3/envs/modeltest/lib/python3.13/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  2.37it/s]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?         Assistant: Hello! I am an artificial intelligence designed to assist with a wide range of tasks. My primary goal is to provide helpful, accurate, and informative responses to your questions. How can I assist you today?\n",
      "User: <image>         Please describe the image.         Assistant: The image depicts a block diagram or flowchart illustrating a communication or data exchange model within a network. It primarily shows two entities labeled as \"Near-RT RIC\" connected through a series of intermediary processes and two nodes labeled as \"E2 Node\". \n",
      "\n",
      "The Near-RT RICs are positioned on either side of the diagram, with the right side of the Near-RT RIC connected to an E2 Node via a red arrow labeled \"RIC SERVICE QUERY\". The left side of the Near-RT RIC connects to an E2 Node through a similar red arrow, also labeled \"RIC SERVICE QUERY\". Between these two entities and the intermediary E2 nodes, there are connections labeled \"RIC Service Update procedure\" pointing in both directions. This signifies a bilateral relationship or interaction between the entities and the E2 nodes involved in a specific communication process. \n",
      "\n",
      "Thus, the diagram indicates a structured flow of service-related queries or updates between Near-RT RICs and E2 Nodes.\n",
      "User: What is best title for the image?         Assistant: \"Flow of RIC Service Queries between Near-RT RICs and E2 Nodes\"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "IMAGENET_MEAN = (0.5, 0.5, 0.5)\n",
    "IMAGENET_STD = (0.5, 0.5, 0.5)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=10, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_data, input_size=384, max_num=10):\n",
    "    image = Image.open(image_data).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "\n",
    "path = 'LiAutoAD/Ristretto-3B'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "image_url = 'Output/O-RAN.WG3.TS.E2AP-R004-v07.00/_page_43_Figure_8.jpeg'\n",
    "# response = requests.get(image_url)\n",
    "# image_data = BytesIO(response.content)\n",
    "pixel_values = load_image(image_url, max_num=10).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# The recommended range for `num_image_token` is 64 to 576, and the value can be adjusted based on task requirements.\n",
    "num_image_token = 256\n",
    "\n",
    "# pure-text conversation\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}         Assistant: {response}')\n",
    "\n",
    "# text-image conversation && multi-round conversation\n",
    "question = '<image>         Please describe the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}         Assistant: {response}')\n",
    "\n",
    "\n",
    "question = 'What is best title for the image?'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}         Assistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f011d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modeltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
